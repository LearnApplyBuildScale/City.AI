{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##<font color='gold'> **S1:TRANSITIONING SCALES**\n",
        "</font>"
      ],
      "metadata": {
        "id": "goUwQYoszmYv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-wpuFbAq65G"
      },
      "source": [
        "###<font color='green'> **ONE_FORMER SEGMENTATION MODEL**\n",
        "</font>\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_architecture.png\n",
        "\" alt=\"drawing\" width=\"1500\"/>\n",
        "\n",
        "\n",
        "# Official Documentation\n",
        "https://huggingface.co/docs/transformers/main/en/model_doc/oneformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color='Green'> **LINK GOOGLE DRIVE**\n",
        "</font>"
      ],
      "metadata": {
        "id": "VRs2uHZv0Sum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "acb6wSeM6Evi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909b0786-d248-4076-ef76-7d447263beba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color='Green'> **[** Setup Environment **]**\n",
        "</font>"
      ],
      "metadata": {
        "id": "giVeC1sZ0jVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgljsR6yq1xI",
        "outputId": "e221dc47-b479-4681-afe3-df89d55df4eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6P-yfwvq9VO"
      },
      "source": [
        "###<font color='Green'> **[** Loading Google Street View **]**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCR3X05rq43Z"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "someSampleImages = [\n",
        "    \"https://1.bp.blogspot.com/-_BWtMqs9QqE/XlVuSzusTbI/AAAAAAAAFXU/FNr-xlxE73QtE4TsxCRbWk4Zwmmd6fP8gCEwYBhgL/s1600/image3.png\",\n",
        "    #\"https://cdn.theatlantic.com/thumbor/EkVEoPqKmjspJIj_earZh-nbvKk=/11x28:1130x657/960x540/media/img/mt/2015/12/Screen_Shot_2015_12_10_at_4.00.43_PM/original.png\",\n",
        "    #\"https://cdn.images.express.co.uk/img/dynamic/59/590x/secondary/google-maps-how-to-get-street-view-google-maps-2421144.jpg?r=1587143071834\",\n",
        "]\n",
        "\n",
        "url = someSampleImages[0] #'https://github.com/SerjoschDuering/datasetTest/raw/main/archImage/8_448px-Johnson_House_Lynchburg_Nov_08.JPG'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzyQFlZ-uVht"
      },
      "source": [
        "###<font color='Green'> **[** Prepare image for the model **]**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY1ABn9FspoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6df03f3-b7ef-4ac1-9446-9f3f411bf671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/oneformer/image_processing_oneformer.py:446: FutureWarning: The `reduce_labels` argument is deprecated and will be removed in v4.27. Please use `do_reduce_labels` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "# the Auto API loads a OneFormerProcessor for us, based on the checkpoint specified\n",
        "processor = AutoProcessor.from_pretrained(\"shi-labs/oneformer_coco_swin_large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Sa7jknyMoG"
      },
      "source": [
        "The inputs are:\n",
        "\n",
        "\n",
        "*   The Image\n",
        "*   The Task (panoptic, instance, semantic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JrDQDiGswe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c38429-0eae-47c2-8d17-a41b966c51dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pixel_values torch.Size([1, 3, 540, 1333])\n",
            "pixel_mask torch.Size([1, 540, 1333])\n",
            "task_inputs torch.Size([1, 77])\n"
          ]
        }
      ],
      "source": [
        "# prepare image for the model\n",
        "image_inputs = processor(images=image, task_inputs=[\"panoptic\"], return_tensors=\"pt\")\n",
        "for k,v in image_inputs.items():\n",
        "  print(k,v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWXBG5OItHzu"
      },
      "source": [
        "**Load model**\n",
        "\n",
        "Next, let's load a model from the HuggingFace [hub](https://huggingface.co/models?other=oneformer). Here we load the OneFormer model with a Swin-large backbone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "qkeFy8jc7c1t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7098cac0-70dc-44fc-e5e6-c2a43de825a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "wqgBRkWz6q16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuqNqhjVtIkG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForUniversalSegmentation\n",
        "\n",
        "model = AutoModelForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_coco_swin_large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrtEAg2zudb-"
      },
      "source": [
        "###<font color='Green'> **[** Forward Pass : Neural Network **]**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdXXqbm_s9Ls"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# forward pass\n",
        "with torch.no_grad():\n",
        "  outputs = model(**image_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUE8Pogktf1B"
      },
      "source": [
        "###<font color='Green'> **[** Visualize Segmentation**]**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sts9kXhMtF4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3176afed-efd0-4162-b9af-ba793d14e16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`label_ids_to_fuse` unset. No instance will be fused.\n"
          ]
        }
      ],
      "source": [
        "img_segmentation_results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDIe802Iz2XP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6196e79a-c238-4c5c-e147-3a3f84c3f60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['segmentation', 'segments_info'])\n",
            "segmentation info -> [{'id': 1, 'label_id': 0, 'was_fused': False, 'score': 0.759748}, {'id': 2, 'label_id': 2, 'was_fused': False, 'score': 0.684714}, {'id': 3, 'label_id': 2, 'was_fused': False, 'score': 0.881031}, {'id': 4, 'label_id': 2, 'was_fused': False, 'score': 0.99112}, {'id': 5, 'label_id': 117, 'was_fused': False, 'score': 0.952258}, {'id': 6, 'label_id': 0, 'was_fused': False, 'score': 0.892504}, {'id': 7, 'label_id': 2, 'was_fused': False, 'score': 0.996726}, {'id': 8, 'label_id': 2, 'was_fused': False, 'score': 0.567867}, {'id': 9, 'label_id': 2, 'was_fused': False, 'score': 0.718453}, {'id': 10, 'label_id': 2, 'was_fused': False, 'score': 0.986467}, {'id': 11, 'label_id': 2, 'was_fused': False, 'score': 0.855201}, {'id': 12, 'label_id': 123, 'was_fused': False, 'score': 0.996802}, {'id': 13, 'label_id': 0, 'was_fused': False, 'score': 0.950064}, {'id': 14, 'label_id': 119, 'was_fused': False, 'score': 0.999105}, {'id': 15, 'label_id': 2, 'was_fused': False, 'score': 0.6442}, {'id': 16, 'label_id': 1, 'was_fused': False, 'score': 0.989557}, {'id': 17, 'label_id': 129, 'was_fused': False, 'score': 0.988829}, {'id': 18, 'label_id': 100, 'was_fused': False, 'score': 0.996385}, {'id': 19, 'label_id': 116, 'was_fused': False, 'score': 0.998471}, {'id': 20, 'label_id': 2, 'was_fused': False, 'score': 0.97668}, {'id': 21, 'label_id': 0, 'was_fused': False, 'score': 0.925324}, {'id': 22, 'label_id': 2, 'was_fused': False, 'score': 0.999221}, {'id': 23, 'label_id': 2, 'was_fused': False, 'score': 0.998793}, {'id': 24, 'label_id': 2, 'was_fused': False, 'score': 0.536731}, {'id': 25, 'label_id': 0, 'was_fused': False, 'score': 0.500834}, {'id': 26, 'label_id': 0, 'was_fused': False, 'score': 0.974289}, {'id': 27, 'label_id': 2, 'was_fused': False, 'score': 0.980145}, {'id': 28, 'label_id': 2, 'was_fused': False, 'score': 0.663182}, {'id': 29, 'label_id': 2, 'was_fused': False, 'score': 0.996989}, {'id': 30, 'label_id': 2, 'was_fused': False, 'score': 0.718303}, {'id': 31, 'label_id': 0, 'was_fused': False, 'score': 0.986514}]\n",
            "segmentation results -> tensor([[19, 19, 19,  ..., 19, 19, 19],\n",
            "        [19, 19, 19,  ..., 19, 19, 19],\n",
            "        [19, 19, 19,  ..., 19, 19, 19],\n",
            "        ...,\n",
            "        [18, 18, 18,  ..., 18, 18, 18],\n",
            "        [18, 18, 18,  ..., 18, 18, 18],\n",
            "        [18, 18, 18,  ..., 18, 18, 18]], dtype=torch.int32)\n"
          ]
        }
      ],
      "source": [
        "print(img_segmentation_results.keys())\n",
        "print(\"segmentation info ->\" , img_segmentation_results[\"segments_info\"])\n",
        "print(\"segmentation results ->\" , img_segmentation_results[\"segmentation\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "metadata": {
        "id": "myxGd6280zmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_panoptic_segmentation(segmentation, segments_info):\n",
        "    # get the used color map\n",
        "    max_segment_id = torch.max(segmentation)\n",
        "    viridis = cm.get_cmap('viridis', max_segment_id + 1)  # Add 1 for zero-indexing\n",
        "    fig, (ax_image, ax_legend) = plt.subplots(1, 2, figsize=(30, 10))  # Create two subplots side by side\n",
        "\n",
        "    ax_image.imshow(segmentation)\n",
        "\n",
        "    # keep track of number of objects\n",
        "    instances_counter = {}\n",
        "    instances_counter_names = {}\n",
        "    instances_shares = {}\n",
        "\n",
        "    # size of output image\n",
        "    total_pixels = segmentation.numel()\n",
        "\n",
        "    handlesLegend = []\n",
        "    colors = {}\n",
        "    # for each segment, draw its legend\n",
        "    for segment in segments_info:\n",
        "        segment_id = segment['id']\n",
        "        if segment_id not in segmentation.unique():\n",
        "            continue  # Skip segments not present in the segmentation\n",
        "\n",
        "        segment_label_id = segment['label_id']\n",
        "        # get name associated to label ID\n",
        "        segment_label = model.config.id2label[segment_label_id]\n",
        "\n",
        "        # get relative share of objects\n",
        "        mask_pixels = torch.sum((segmentation == segment_id))\n",
        "        mask_share = (mask_pixels.item() / total_pixels) * 100\n",
        "\n",
        "        # count objects\n",
        "        try:\n",
        "            instances_counter_names[segment_label] += 1\n",
        "            instances_shares[segment_label] += round(mask_share, 2)\n",
        "        except:\n",
        "            instances_counter_names[segment_label] = 1\n",
        "            instances_shares[segment_label] = round(mask_share, 2)\n",
        "\n",
        "        # Get color from the segmented image based on segment ID\n",
        "        if segment_id not in colors:\n",
        "            color = viridis(segment_id / max_segment_id)\n",
        "            colors[segment_id] = color\n",
        "        else:\n",
        "            color = colors[segment_id]\n",
        "\n",
        "        # Create legend patch with corresponding color\n",
        "        label = f\"{segment_label}-count: {instances_counter_names[segment_label]} | {round(instances_shares[segment_label],1)}%\"\n",
        "        handlesLegend.append(mpatches.Patch(color=color, label=label))\n",
        "\n",
        "    ax_legend.legend(handles=handlesLegend)\n",
        "\n",
        "    # Remove axis labels and ticks from the legend subplot\n",
        "    ax_legend.set_axis_off()\n",
        "\n",
        "    return instances_counter_names, instances_shares, plt\n",
        "\n",
        "counter_dict, shares_dict, plt_instance = draw_panoptic_segmentation(**img_segmentation_results)\n"
      ],
      "metadata": {
        "id": "2pVvpbG1FL8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwgKZWRPynYN"
      },
      "source": [
        "The model is able to distinguish between individual objects. using the instance_counter dictionary we can easily count objects (e.g. number of cars) or compute their relative share (e.g. percentage of greenery)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color='gold'> **S2: DATA STRUCTURING FOR APPLICATION**\n",
        "</font>"
      ],
      "metadata": {
        "id": "lCwmsVEZ4de0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCW1r4cNKdbQ"
      },
      "source": [
        "###<font color='Green'> **[** Structuring Data for App**]**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJmc-rffIzaV",
        "outputId": "f7311b25-820e-4ab3-8d59-b6f0bffd9a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.0)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "from transformers import AutoProcessor\n",
        "from transformers import AutoModelForUniversalSegmentation\n",
        "import torch\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import matplotlib.patches as mpatches\n",
        "import numpy as np\n",
        "!pip install -U scipy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####<font color='Green'> **[** Function & Loop **]**\n",
        "</font>"
      ],
      "metadata": {
        "id": "GDlTnAn127ED"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIIuZSYIH4EY"
      },
      "outputs": [],
      "source": [
        "# load all images from \"someSampleImages\"\n",
        "images_for_segmentation = []\n",
        "\n",
        "for img_url in someSampleImages:\n",
        "  image = Image.open(requests.get(img_url, stream=True).raw)\n",
        "  images_for_segmentation.append(image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLh0GUsaWpA0"
      },
      "outputs": [],
      "source": [
        "# create a color iamge from segmentationr esults\n",
        "def tensor_to_segmented_image(segmented_tensor, num_classes=None):\n",
        "    # Converting PyTorch tensor to a NumPy array\n",
        "    segmented_array = segmented_tensor.numpy().astype(np.uint8)\n",
        "\n",
        "    # Assigning a color to each class\n",
        "    unique_values = np.unique(segmented_array)\n",
        "    if num_classes is None:\n",
        "        num_classes = len(unique_values)\n",
        "    colors = np.random.randint(0, 255, size=(num_classes, 3), dtype=np.uint8)\n",
        "\n",
        "    # Map unique values in the segmented_array to indices\n",
        "    value_to_index_map = {value: index for index, value in enumerate(unique_values)}\n",
        "    mapped_array = np.vectorize(value_to_index_map.get)(segmented_array)\n",
        "\n",
        "    # Apply colors to the mapped_array\n",
        "    colored_segmentation = colors[mapped_array]\n",
        "\n",
        "    # Converting NumPy array to PIL Image\n",
        "    segmented_image = Image.fromarray(colored_segmentation)\n",
        "    return segmented_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSIygIKmRqsD"
      },
      "outputs": [],
      "source": [
        "def draw_panoptic_segmentation(segmentation, segments_info):\n",
        "    # get the used color map\n",
        "    viridis = cm.get_cmap('viridis', torch.max(segmentation))\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(segmentation)\n",
        "\n",
        "    # keep track of number of objects\n",
        "    instances_counter = {}\n",
        "    instances_counter_names = {}\n",
        "    instances_shares = {}\n",
        "\n",
        "    # size of output image\n",
        "    total_pixels = segmentation.numel()\n",
        "\n",
        "    handlesLegend = []\n",
        "    # for each segment, draw its legend\n",
        "    for segment in segments_info:\n",
        "        segment_id = segment['id']\n",
        "        segment_label_id = segment['label_id']\n",
        "        # get name associated to lable ID\n",
        "        segment_label = model.config.id2label[segment_label_id]\n",
        "\n",
        "        # get relative share of objects\n",
        "        mask_pixels = torch.sum((segmentation == segment_id))\n",
        "        mask_share = (mask_pixels.item() / total_pixels) * 100\n",
        "\n",
        "        # count objects\n",
        "        try:\n",
        "          instances_counter_names[segment_label] += 1\n",
        "          instances_shares[segment_label] += round(mask_share,2)\n",
        "        except:\n",
        "          instances_counter_names[segment_label] = 1\n",
        "          instances_shares[segment_label] = round(mask_share, 2)\n",
        "\n",
        "\n",
        "    # Adjusted Legend\n",
        "    for k_name, i_num in instances_counter_names.items():\n",
        "      label = f\"{k_name}-count: {i_num} | {round(instances_shares[k_name],1)}%\"\n",
        "      handlesLegend.append(mpatches.Patch(color=\"white\", label=label))\n",
        "\n",
        "    print(instances_counter_names)\n",
        "    print(instances_shares)\n",
        "    ax.legend(handles=handlesLegend)\n",
        "\n",
        "    # create a PIL image from the tensor/array -> this can be used in gradio mire easially\n",
        "    segmented_image = tensor_to_segmented_image(segmentation)\n",
        "    print(segmented_image)\n",
        "\n",
        "    return instances_counter_names, instances_shares, segmented_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8-HxhvJHakL"
      },
      "outputs": [],
      "source": [
        "def getImageData(img, model, processor):\n",
        "    # encode image\n",
        "    image_inputs = processor(images=img, task_inputs=[\"panoptic\"], return_tensors=\"pt\")\n",
        "\n",
        "    # forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**image_inputs)\n",
        "\n",
        "    # get results\n",
        "    img_segmentation_results = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
        "\n",
        "    # call result processing function\n",
        "    counter_dict, shares_dict, segmentation_map = draw_panoptic_segmentation(**img_segmentation_results)\n",
        "\n",
        "    return counter_dict, shares_dict, segmentation_map\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ffKEChiLZq3"
      },
      "source": [
        "##<font color='gold'> **S3: GRADIO APP**\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###<font color='Green'> **[** Launch App**]**\n",
        "</font>"
      ],
      "metadata": {
        "id": "jHRwRizL49d9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_V6jpzNLoXN"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --upgrade\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MMGjY_kv457J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def segment_image(image):\n",
        "    counter_dict, shares_dict, segmentation_map = getImageData(image,model,processor)\n",
        "\n",
        "    # Prepare data for the table\n",
        "    table_data = []\n",
        "    for class_name in counter_dict.keys():\n",
        "        instance_count = counter_dict[class_name]\n",
        "        share = shares_dict[class_name]\n",
        "        table_data.append([class_name, instance_count, share])\n",
        "\n",
        "    # Create a pandas DataFrame for the table\n",
        "    table_df = pd.DataFrame(table_data, columns=[\"Class Name\", \"Instance Count\", \"Share\"])\n",
        "\n",
        "    return segmentation_map, table_df\n",
        "\n",
        "# Define Gradio input and output components\n",
        "\n",
        "image_input = gr.Image(label=\"Input Image\")\n",
        "image_output = gr.Image(type=\"pil\", label=\"Segmented Image\")\n",
        "table_output = gr.Dataframe(type=\"pandas\", label=\"Detected Classes\")\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(fn=segment_image,\n",
        "                     inputs=[image_input],\n",
        "                     outputs=[image_output, table_output],\n",
        "                     title=\"Image Segmentation\",\n",
        "                     description=\"Upload an image to segment it using the pretrained model.\")\n",
        "\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "KzvlDmuUWFcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gradio as gr\n",
        "\n",
        "# Mock function to simulate getImageData function\n",
        "def getImageData(image, model, processor):\n",
        "    # This function is not defined in the provided code snippet\n",
        "    # It should be replaced with the actual function call to get segmentation data\n",
        "    pass\n",
        "\n",
        "def segment_image(image):\n",
        "    counter_dict, shares_dict, segmentation_map = getImageData(image, model, processor)\n",
        "\n",
        "    # Prepare data for the table and bar chart\n",
        "    table_data = []\n",
        "    bar_chart_data = {\"Class Name\": [], \"Instance Count\": []}\n",
        "    for class_name in counter_dict.keys():\n",
        "        instance_count = counter_dict[class_name]\n",
        "        share = shares_dict[class_name]\n",
        "        table_data.append([class_name, instance_count, share])\n",
        "        bar_chart_data[\"Class Name\"].append(class_name)\n",
        "        bar_chart_data[\"Instance Count\"].append(instance_count)\n",
        "\n",
        "    # Create a pandas DataFrame for the table\n",
        "    table_df = pd.DataFrame(table_data, columns=[\"Class Name\", \"Instance Count\", \"Share\"])\n",
        "\n",
        "    # Create a bar chart from the instance counts\n",
        "    bar_chart_df = pd.DataFrame(bar_chart_data)\n",
        "\n",
        "    return segmentation_map, table_df, bar_chart_df\n",
        "\n",
        "# Define Gradio input and output components\n",
        "image_input = gr.Image(label=\"Input Image\")\n",
        "image_output = gr.Image(type=\"pil\", label=\"Segmented Image\")\n",
        "table_output = gr.Dataframe(type=\"pandas\", label=\"Detected Classes\")\n",
        "bar_chart_output = gr.BarPlot(label=\"Instance Count Bar Chart\")\n",
        "\n",
        "\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(fn=segment_image,\n",
        "                     inputs=image_input,\n",
        "                     outputs=[image_output,table_output, bar_chart_output],\n",
        "                     title=\"Image Segmentation\",\n",
        "                     description=\"Upload an image to segment it using the pretrained model.\")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "uB1Xzb32QzkH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "VRs2uHZv0Sum"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}